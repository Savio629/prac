{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer"
      ],
      "metadata": {
        "id": "XFWIPTxsW8bj"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download NLTK resources (if needed)\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "id": "UT56iURBXA5w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenization\n",
        "text = \"Python is a general-purpose language, used to create a range of applications, including data science, software and web development, automation, and improving the ease of everyday tasks.\"\n",
        "tokens = word_tokenize(text)\n",
        "print(\"Tokenization:\", tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "mIbg01d1XA_g",
        "outputId": "31c7a08d-3142-49b1-ce0c-750d6977f951"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenization: ['Python', 'is', 'a', 'general-purpose', 'language', ',', 'used', 'to', 'create', 'a', 'range', 'of', 'applications', ',', 'including', 'data', 'science', ',', 'software', 'and', 'web', 'development', ',', 'automation', ',', 'and', 'improving', 'the', 'ease', 'of', 'everyday', 'tasks', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Stopword removal\n",
        "stop_words = set(stopwords.words('english'))\n",
        "filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
        "print(\"Stopword removal:\", filtered_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "g7O-9dhHXBCz",
        "outputId": "ba15389d-eeb3-47f4-96cd-2d50530112bf"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stopword removal: ['Python', 'general-purpose', 'language', ',', 'used', 'create', 'range', 'applications', ',', 'including', 'data', 'science', ',', 'software', 'web', 'development', ',', 'automation', ',', 'improving', 'ease', 'everyday', 'tasks', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Stemming\n",
        "ps = PorterStemmer()\n",
        "stemmed_tokens = [ps.stem(word) for word in filtered_tokens]\n",
        "print(\"Stemming:\", stemmed_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "pjaBBXwgXBGC",
        "outputId": "a8141cc0-1ace-4495-fd10-15ba3f889a77"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stemming: ['python', 'general-purpos', 'languag', ',', 'use', 'creat', 'rang', 'applic', ',', 'includ', 'data', 'scienc', ',', 'softwar', 'web', 'develop', ',', 'autom', ',', 'improv', 'eas', 'everyday', 'task', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Lemmatization\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "lemmatized_tokens = [lemmatizer.lemmatize(word) for word in filtered_tokens]\n",
        "print(\"Lemmatization:\", lemmatized_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "s2BwbQaBXBJC",
        "outputId": "228b100d-e740-4ff5-9b38-9904dbf348b2"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lemmatization: ['Python', 'general-purpose', 'language', ',', 'used', 'create', 'range', 'application', ',', 'including', 'data', 'science', ',', 'software', 'web', 'development', ',', 'automation', ',', 'improving', 'ease', 'everyday', 'task', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "# Load spaCy English model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "# Tokenization, Lemmatization, Part-of-Speech Tagging\n",
        "doc = nlp(\"Computer vision is a field of computer science that focuses on enabling computers to identify and understand objects and people in images and videos. Like other types of AI, computer vision seeks to perform and automate tasks that replicate human capabilities.\")\n",
        "tokens = [token.text for token in doc]\n",
        "lemmatized_tokens = [token.lemma_ for token in doc]\n",
        "pos_tags = [(token.text, token.pos_) for token in doc]\n",
        "print(\"Tokenization:\", tokens)\n",
        "print(\"Lemmatization:\", lemmatized_tokens)\n",
        "print(\"Part-of-Speech Tagging:\",pos_tags)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "BMBGiSzhXBMB",
        "outputId": "fcbda547-d815-414d-9092-400fedb9f3fa"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenization: ['Computer', 'vision', 'is', 'a', 'field', 'of', 'computer', 'science', 'that', 'focuses', 'on', 'enabling', 'computers', 'to', 'identify', 'and', 'understand', 'objects', 'and', 'people', 'in', 'images', 'and', 'videos', '.', 'Like', 'other', 'types', 'of', 'AI', ',', 'computer', 'vision', 'seeks', 'to', 'perform', 'and', 'automate', 'tasks', 'that', 'replicate', 'human', 'capabilities', '.']\n",
            "Lemmatization: ['computer', 'vision', 'be', 'a', 'field', 'of', 'computer', 'science', 'that', 'focus', 'on', 'enable', 'computer', 'to', 'identify', 'and', 'understand', 'object', 'and', 'people', 'in', 'image', 'and', 'video', '.', 'like', 'other', 'type', 'of', 'AI', ',', 'computer', 'vision', 'seek', 'to', 'perform', 'and', 'automate', 'task', 'that', 'replicate', 'human', 'capability', '.']\n",
            "Part-of-Speech Tagging: [('Computer', 'NOUN'), ('vision', 'NOUN'), ('is', 'AUX'), ('a', 'DET'), ('field', 'NOUN'), ('of', 'ADP'), ('computer', 'NOUN'), ('science', 'NOUN'), ('that', 'PRON'), ('focuses', 'VERB'), ('on', 'ADP'), ('enabling', 'VERB'), ('computers', 'NOUN'), ('to', 'PART'), ('identify', 'VERB'), ('and', 'CCONJ'), ('understand', 'VERB'), ('objects', 'NOUN'), ('and', 'CCONJ'), ('people', 'NOUN'), ('in', 'ADP'), ('images', 'NOUN'), ('and', 'CCONJ'), ('videos', 'NOUN'), ('.', 'PUNCT'), ('Like', 'ADP'), ('other', 'ADJ'), ('types', 'NOUN'), ('of', 'ADP'), ('AI', 'PROPN'), (',', 'PUNCT'), ('computer', 'NOUN'), ('vision', 'NOUN'), ('seeks', 'VERB'), ('to', 'PART'), ('perform', 'VERB'), ('and', 'CCONJ'), ('automate', 'ADJ'), ('tasks', 'NOUN'), ('that', 'PRON'), ('replicate', 'VERB'), ('human', 'ADJ'), ('capabilities', 'NOUN'), ('.', 'PUNCT')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "# Word Frequency Counting\n",
        "word_freq = Counter(tokens)\n",
        "# Named Entity Recognition\n",
        "named_entities = [(entity.text, entity.label_) for entity in doc.ents]\n",
        "# Bag of Words Representation\n",
        "bag_of_words = {token.text: token.is_alpha for token in doc}\n",
        "print(\"Word Frequency Count:\", word_freq)\n",
        "print(\"Named Entities:\", named_entities)\n",
        "print(\"Bag of Words Representation:\", bag_of_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "2M34jOnhYL-s",
        "outputId": "be37258e-e914-4f1d-f21f-7eb816bb7faf"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word Frequency Count: Counter({'and': 4, 'vision': 2, 'of': 2, 'computer': 2, 'that': 2, 'to': 2, '.': 2, 'Computer': 1, 'is': 1, 'a': 1, 'field': 1, 'science': 1, 'focuses': 1, 'on': 1, 'enabling': 1, 'computers': 1, 'identify': 1, 'understand': 1, 'objects': 1, 'people': 1, 'in': 1, 'images': 1, 'videos': 1, 'Like': 1, 'other': 1, 'types': 1, 'AI': 1, ',': 1, 'seeks': 1, 'perform': 1, 'automate': 1, 'tasks': 1, 'replicate': 1, 'human': 1, 'capabilities': 1})\n",
            "Named Entities: [('AI', 'ORG')]\n",
            "Bag of Words Representation: {'Computer': True, 'vision': True, 'is': True, 'a': True, 'field': True, 'of': True, 'computer': True, 'science': True, 'that': True, 'focuses': True, 'on': True, 'enabling': True, 'computers': True, 'to': True, 'identify': True, 'and': True, 'understand': True, 'objects': True, 'people': True, 'in': True, 'images': True, 'videos': True, '.': False, 'Like': True, 'other': True, 'types': True, 'AI': True, ',': False, 'seeks': True, 'perform': True, 'automate': True, 'tasks': True, 'replicate': True, 'human': True, 'capabilities': True}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m textblob.download_corpora\n",
        "!pip install googletrans==4.0.0-rc1"
      ],
      "metadata": {
        "id": "vrTOctbldlc1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from textblob import TextBlob\n",
        "text = \"I Love AI.\"\n",
        "blob = TextBlob(text)\n",
        "# Sentiment Analysis\n",
        "print(\"Sentiment Analysis:\", blob.sentiment)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "UehsIAg2YMBW",
        "outputId": "62e5a2e0-135f-47d8-a623-479b0c2894d4"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentiment Analysis: Sentiment(polarity=0.5, subjectivity=0.6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from googletrans import Translator\n",
        "text = \"I Love AI.\"\n",
        "\n",
        "# Detect language\n",
        "translator = Translator()\n",
        "detected_language = translator.detect(text).lang\n",
        "print(\"Detected Language:\", detected_language)\n",
        "\n",
        "# Translate to French\n",
        "if detected_language != 'fr':  # Check if the detected language is not already French\n",
        "    translated_text = translator.translate(text, dest='fr').text\n",
        "    print(\"Translation to French:\", translated_text)\n",
        "else:\n",
        "    print(\"Text is already in French.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "sdBcbks8fiX9",
        "outputId": "8769dbbd-7313-4d4d-a8d7-53c92a64c1e7"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Detected Language: en\n",
            "Translation to French: J'adore ai.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"I lov AI.\"\n",
        "blob = TextBlob(text)\n",
        "\n",
        "# Correct the text\n",
        "corrected_text = blob.correct()\n",
        "\n",
        "print(\"Original Text:\", text)\n",
        "print(\"Corrected Text:\", corrected_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "EZKjr_hefigv",
        "outputId": "b63493a5-a16f-4903-f23b-bcf42aa35dfa"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Text: I lov AI.\n",
            "Corrected Text: I love of.\n"
          ]
        }
      ]
    }
  ]
}